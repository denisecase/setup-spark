# Create a new Python notebook in Databricks
# shortcuts:
#   SHIFT + ENTER to run
#   CTRL SHIFT P brings up print dialog
# 

print("hello")
print(2+4)
x = ["hello","big", "data", "world"]

print(x)

print(map(lambda i:len(i),x))

# oops - cast the iterable object to a list
print(list(map(lambda i:len(i),x)))

# or use the iterable unpacking operator, * (** for dict)
print(*map(lambda i:len(i),x))

# iterate over the iterable
for s in map(lambda i: len(i),x):
  print(s)
  
# filter
print(*filter(lambda i:len(i)>3, x))

# filter and sort
print(*sorted(filter(lambda i:len(i)>3, x)) )

# filter, sort, count
print( len(sorted(filter(lambda i:len(i)>3, x)) ))

# reduce
from functools import reduce
print(reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]))

# try summing the lengths
print(reduce(lambda acc,i: acc+len(i), x) )

# oops - from the iterator, x, it assumes str
# set optional initializer to 0 to set type to int - now we can add
print(reduce(lambda acc,i: acc+len(i), x, 0) )


#---------------- Spark RDD map-reduce

rdd = sc.parallelize(x,4)
print type(rdd)

# map to intermediate key-value pairs
kv = rdd.map(lambda w: (w,1))
print(kv.collect())

# reduce by key
res = kv.reduceByKey(lambda x,y: x+y)
print(res.collect())

# ---------------- Spark RDD all in one
from operator import add

print((rdd.map(lambda w: (w,1)).reduceByKey(add).collect()))





